{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa9XM265sd0EGaxWt4mEDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheophileZuber/2024_MLEES/blob/main/LSTM_Model_Monkey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all packages"
      ],
      "metadata": {
        "id": "Ox21tVYukqpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az36EJC6kYtw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import psutil  # For memory usage tracking\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Check if drive is already mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data formating and creation of the training, validation and test sets"
      ],
      "metadata": {
        "id": "KaQ6Mw0nm2-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to creat sequences of data and transform/normalise timestamps"
      ],
      "metadata": {
        "id": "2K2R3AtYndYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Function to create sequences with time-window padding\n",
        "def create_time_window_sequences(data, window_hours=6, behavior_columns_range=(8, 15), pad_length=240):\n",
        "    sequences = []  # To store the sequences of input features\n",
        "    targets = []    # To store the target behavior values corresponding to each sequence\n",
        "\n",
        "    # Ensure 'data' is a DataFrame\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "        raise ValueError(\"Expected input 'data' to be a DataFrame\")\n",
        "\n",
        "    # Ensure behavior_columns_range is a tuple\n",
        "    if not isinstance(behavior_columns_range, tuple):\n",
        "        raise ValueError(\"behavior_columns_range must be a tuple\")\n",
        "\n",
        "    # Convert 'timestamp' column to datetime objects (important for time-based operations)\n",
        "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
        "\n",
        "    # Extract useful time features: hour of the day and day of the week\n",
        "    data['hour'] = data['timestamp'].dt.hour\n",
        "    data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
        "\n",
        "    # Create cyclic (sinusoidal and cosinusoidal) time features to capture periodic patterns\n",
        "    # This helps the model understand cyclic patterns such as 24-hour cycles or weekly patterns\n",
        "    data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
        "    data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
        "    data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
        "    data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
        "\n",
        "    # Sort the data by individual ID and timestamp to ensure chronological order within each group\n",
        "    data.sort_values(by=['Ind_ID', 'timestamp'], inplace=True)\n",
        "\n",
        "    # Define the columns that represent behavior (target) data and feature (input) data\n",
        "    behavior_columns = list(data.columns[behavior_columns_range[0]:behavior_columns_range[1]])\n",
        "    feature_columns = [\n",
        "        'speed_ms', 'stepLenght', 'turnAngle_sin', 'turnAngle_cos',  # Movement features\n",
        "        'hour_sin', 'hour_cos', 'day_sin', 'day_cos'                 # Cyclic time features\n",
        "    ] + list(data.columns[15:25])  # Landcover features\n",
        "\n",
        "    # Group the data by individual IDs to process each individual's data separately\n",
        "    grouped = data.groupby('Ind_ID')\n",
        "\n",
        "    # Iterate over each group (individual) in the dataset\n",
        "    for _, group in grouped:\n",
        "        group = group.reset_index(drop=True)  # Reset index to avoid issues with indexing within the group\n",
        "        start_idx = 0  # Initialize the starting index for creating windows within the group\n",
        "\n",
        "        # Create overlapping sequences within the group based on the defined time window\n",
        "        while start_idx < len(group):\n",
        "            # Define the end of the time window based on the start index and specified window size\n",
        "            end_time = group.loc[start_idx, 'timestamp'] + pd.Timedelta(hours=window_hours)\n",
        "\n",
        "            # Select data within the time window\n",
        "            window_data = group[(group['timestamp'] >= group.loc[start_idx, 'timestamp']) &\n",
        "                                (group['timestamp'] < end_time)]\n",
        "\n",
        "            # Ensure the window has at least two records to form a valid sequence\n",
        "            if len(window_data) >= 2:\n",
        "                # Extract the input features for the sequence and the target behavior values\n",
        "                seq = window_data[feature_columns].values  # Sequence of input features (numpy array)\n",
        "                target = window_data[behavior_columns].values[-1]  # Target values from the last row in the window\n",
        "                sequences.append(seq)  # Add the sequence to the list\n",
        "                targets.append(target)  # Add the target to the list\n",
        "\n",
        "            start_idx += 1  # Move the starting index to create the next window\n",
        "\n",
        "    # Pad sequences to ensure uniform length (maxlen) for model input consistency\n",
        "    padded_sequences = pad_sequences(\n",
        "        sequences,                # List of sequences\n",
        "        maxlen=pad_length,        # Desired length of the sequences after padding\n",
        "        dtype='float32',          # Data type of the sequences\n",
        "        padding='post',           # Pad at the end of the sequence if shorter than maxlen\n",
        "        truncating='post'         # Truncate at the end if sequence is longer than maxlen\n",
        "    )\n",
        "\n",
        "    # Return the padded sequences and corresponding targets as numpy arrays\n",
        "    return np.array(padded_sequences), np.array(targets)\n"
      ],
      "metadata": {
        "id": "iwzzpuozm2p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data by individuals and timestamp"
      ],
      "metadata": {
        "id": "-IvH20EMm2fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Function to split data chronologically by individual\n",
        "def split_chronologically_by_individual(data, train_ratio=0.5, val_ratio=0.25, random_state=42):\n",
        "    # Ensure data is sorted chronologically within each individual ID group\n",
        "    data = data.sort_values(by=['Ind_ID', 'timestamp'])  # Sort by individual ID and timestamp to maintain order\n",
        "\n",
        "    # Check if the input DataFrame is empty and handle it gracefully\n",
        "    if data.empty:\n",
        "        print(\"Warning: Received an empty DataFrame during split.\")\n",
        "        return data, data, data  # Return empty DataFrames for train, validation, and test splits\n",
        "\n",
        "    # Get unique individual IDs\n",
        "    unique_ids = data['Ind_ID'].unique()  # Extract unique IDs to split data by individuals\n",
        "\n",
        "    # Set a random seed for reproducibility (consistent shuffling results across runs)\n",
        "    np.random.seed(random_state)\n",
        "    np.random.shuffle(unique_ids)  # Randomly shuffle the individual IDs to ensure a diverse split\n",
        "\n",
        "    # Calculate the number of individuals for each dataset split based on the provided ratios\n",
        "    train_size = int(len(unique_ids) * train_ratio)  # Number of individuals in the training set\n",
        "    val_size = int(len(unique_ids) * val_ratio)      # Number of individuals in the validation set\n",
        "\n",
        "    # Split the shuffled unique IDs into training, validation, and test sets\n",
        "    train_ids = unique_ids[:train_size]                                # First part for training\n",
        "    val_ids = unique_ids[train_size:train_size + val_size]             # Next part for validation\n",
        "    test_ids = unique_ids[train_size + val_size:]                      # Remaining part for testing\n",
        "\n",
        "    # Extract data for each split by filtering based on the assigned individual IDs\n",
        "    train_data = data[data['Ind_ID'].isin(train_ids)]  # Filter data belonging to train IDs\n",
        "    val_data = data[data['Ind_ID'].isin(val_ids)]      # Filter data belonging to validation IDs\n",
        "    test_data = data[data['Ind_ID'].isin(test_ids)]    # Filter data belonging to test IDs\n",
        "\n",
        "    # Return the three subsets as DataFrames\n",
        "    return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "yqzzPjyvm3z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function which generate the chunks for training, validation and testing"
      ],
      "metadata": {
        "id": "vAbROnL9oPwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Generator to process data in chunks and split into train/val/test sets\n",
        "def data_generator_fixed_split(file_path, chunk_size=1024, window_hours=6, pad_length=240):\n",
        "\n",
        "    # Use pandas' chunk reader to process the large CSV file in manageable chunks\n",
        "    chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)  # Create an iterator for reading CSV chunks\n",
        "\n",
        "    # Loop through each chunk of data from the CSV file\n",
        "    for chunk in chunk_iter:\n",
        "        # Check if the current chunk is empty and skip it if necessary\n",
        "        if chunk.empty:\n",
        "            print(f\"Warning: Chunk {chunk} is empty.\")  # Print a warning if the chunk is empty\n",
        "            continue  # Skip to the next chunk\n",
        "\n",
        "        # Split the current chunk into train, validation, and test sets based on individual IDs\n",
        "        train_data, val_data, test_data = split_chronologically_by_individual(chunk)\n",
        "\n",
        "        # Create time-window sequences for each split (train, validation, and test)\n",
        "        X_train_chunk, y_train_chunk = create_time_window_sequences(train_data, window_hours, pad_length=pad_length)\n",
        "        X_val_chunk, y_val_chunk = create_time_window_sequences(val_data, window_hours, pad_length=pad_length)\n",
        "        X_test_chunk, y_test_chunk = create_time_window_sequences(test_data, window_hours, pad_length=pad_length)\n",
        "\n",
        "        # Ensure that sequences are not empty before yielding\n",
        "        if X_train_chunk.size == 0 or X_val_chunk.size == 0 or X_test_chunk.size == 0:\n",
        "            print(f\"Skipping empty sequence chunk {chunk}.\")  # Print a warning if sequences are empty\n",
        "            continue  # Skip this chunk if any sequence set is empty\n",
        "\n",
        "        # Yield the processed data for the current chunk, which allows for batch processing in training\n",
        "        yield X_train_chunk, X_val_chunk, X_test_chunk, y_train_chunk, y_val_chunk, y_test_chunk\n"
      ],
      "metadata": {
        "id": "Mi4VTeGsoQMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate training, validation and test sets"
      ],
      "metadata": {
        "id": "Mk7DWAgkoRSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths and variables\n",
        "file_path = \"/content/drive/MyDrive/data/AarhusAsseblief_sub.csv\"\n",
        "output_path = \"/content/drive/MyDrive/data_chunks/\"\n",
        "directory = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "full_output_path = os.path.join(output_path, directory)\n",
        "\n",
        "if not os.path.exists(full_output_path):\n",
        "    os.makedirs(full_output_path)\n",
        "\n",
        "# Process and save data in chunks\n",
        "for i, (X_train_chunk, X_val_chunk, X_test_chunk, y_train_chunk, y_val_chunk, y_test_chunk) in enumerate(\n",
        "        data_generator_fixed_split(file_path, chunk_size=5000)):\n",
        "\n",
        "    np.save(f\"{full_output_path}/X_train_chunk_{i}.npy\", X_train_chunk)\n",
        "    np.save(f\"{full_output_path}/X_val_chunk_{i}.npy\", X_val_chunk)\n",
        "    np.save(f\"{full_output_path}/X_test_chunk_{i}.npy\", X_test_chunk)\n",
        "    np.save(f\"{full_output_path}/y_train_chunk_{i}.npy\", y_train_chunk)\n",
        "    np.save(f\"{full_output_path}/y_val_chunk_{i}.npy\", y_val_chunk)\n",
        "    np.save(f\"{full_output_path}/y_test_chunk_{i}.npy\", y_test_chunk)\n",
        "\n",
        "    print(f\"Chunk {i} processed and saved. Memory usage: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "print(\"Data processing completed successfully!\")"
      ],
      "metadata": {
        "id": "xVJN8I_4oRH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer and model definition"
      ],
      "metadata": {
        "id": "fXTJTAtJm5pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention layer definition"
      ],
      "metadata": {
        "id": "Z6pZcvZjkrL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow for building custom layers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a custom feature attention layer that applies attention mechanisms to input features\n",
        "class FeatureAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom Keras layer that applies attention to input features,\n",
        "    dynamically weighting their importance during training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FeatureAttention, self).__init__(**kwargs)  # Call parent class initializer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Defines the weights for the attention mechanism.\n",
        "        \"\"\"\n",
        "        # Initialize the attention weights matrix (W) with shape (num_features, num_features)\n",
        "        self.W = self.add_weight(\n",
        "            shape=(input_shape[-1], input_shape[-1]),  # Shape is (num_features, num_features)\n",
        "            initializer=\"normal\",  # Use normal distribution for initialization\n",
        "            trainable=True  # Allow the weights to be updated during training\n",
        "        )\n",
        "        super(FeatureAttention, self).build(input_shape)  # Finalize the build process\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Applies the attention mechanism to the input data.\n",
        "\n",
        "        \"\"\"\n",
        "        # Calculate attention scores using a softmax over the transformed features\n",
        "        # (batch_size, time_steps, num_features) x (num_features, num_features) -> (batch_size, time_steps, num_features)\n",
        "        score = tf.nn.softmax(tf.matmul(x, self.W), axis=-1)  # Apply softmax along the feature axis\n",
        "\n",
        "        # Multiply each input feature by its corresponding attention score\n",
        "        # Element-wise multiplication: (batch_size, time_steps, num_features) * (batch_size, time_steps, num_features)\n",
        "        context = x * score\n",
        "\n",
        "        # Aggregate the attention-weighted features over all time steps (reduce along axis 1)\n",
        "        # Resulting shape: (batch_size, num_features)\n",
        "        return tf.reduce_sum(context, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "a6xwJvqIkrkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "7Vev_3IPkr6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the LSTM model with the Attention layer\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True,input_shape=(240, 18)),\n",
        "    Dropout(0.5),  # Regularization to prevent overfitting\n",
        "    FeatureAttention(),   # Custom Attention Layer\n",
        "    Dense(7, activation='softmax')  # Output layer for classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "cDg57LKEksbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile model"
      ],
      "metadata": {
        "id": "vUfweDd6l4tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model before training\n",
        "model.compile(\n",
        "    optimizer='adam',                 # Optimizer for weight updates\n",
        "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
        "    metrics=['accuracy']              # Track accuracy during training\n",
        ")\n"
      ],
      "metadata": {
        "id": "LjpSXZ-zl45S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up paths to datachunks, checkpoint of the model, early_stopping and logs"
      ],
      "metadata": {
        "id": "W8X-x4-imGjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to saved chunks\n",
        "train_path = \"/content/drive/MyDrive/data_chunks/2024-12-05/\"\n",
        "val_path = \"/content/drive/MyDrive/data_chunks/2024-12-05/\"\n",
        "test_path = \"/content/drive/MyDrive/data_chunks/2024-12-05/\"\n",
        "base_output_path = \"/content/drive/MyDrive/Model_results/\"\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Checkpoints/lstm_model.keras', save_best_only=True, monitor='val_loss')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "csv_logger = CSVLogger('/content/drive/MyDrive/logs/training_log.csv')\n"
      ],
      "metadata": {
        "id": "6w0WAm81mGUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "eKMOWs5mmPnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_chunks = 227\n",
        "num_epochs = 1\n",
        "\n",
        "# Initialize lists to store loss and accuracy metrics\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Train and validate on each chunk\n",
        "for i in range(num_chunks):\n",
        "    X_train_chunk = np.load(f\"{train_path}X_train_chunk_{i}.npy\")\n",
        "    y_train_chunk = np.load(f\"{train_path}y_train_chunk_{i}.npy\")\n",
        "    X_val_chunk = np.load(f\"{val_path}X_val_chunk_{i}.npy\")\n",
        "    y_val_chunk = np.load(f\"{val_path}y_val_chunk_{i}.npy\")\n",
        "\n",
        "    # Train the model on the current chunk and store history\n",
        "    history = model.fit(X_train_chunk, y_train_chunk, epochs=num_epochs, batch_size=32,\n",
        "                        validation_data=(X_val_chunk, y_val_chunk),\n",
        "                        verbose=1, callbacks=[checkpoint, early_stopping, csv_logger])\n",
        "\n",
        "    # Append metrics from this chunk\n",
        "    train_losses.extend(history.history['loss'])\n",
        "    val_losses.extend(history.history['val_loss'])\n",
        "    train_accuracies.extend(history.history['accuracy'])\n",
        "    val_accuracies.extend(history.history['val_accuracy'])\n",
        "\n",
        "# Save training history to CSV\n",
        "history_df = pd.DataFrame({\n",
        "    'epoch': np.arange(len(train_losses)),\n",
        "    'train_loss': train_losses,\n",
        "    'val_loss': val_losses,\n",
        "    'train_accuracy': train_accuracies,\n",
        "    'val_accuracy': val_accuracies\n",
        "})\n",
        "\n",
        "history_df.to_csv(os.path.join(base_output_path, 'training_history.csv'), index=False)"
      ],
      "metadata": {
        "id": "FgnusfBpmRrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the learning of the model"
      ],
      "metadata": {
        "id": "4cJufiQamgNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\n",
        "plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curve')\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df['epoch'], history_df['train_accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_df['epoch'], history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig(os.path.join(base_output_path, 'learning_curves.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "b-zdyDa4mf1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the model"
      ],
      "metadata": {
        "id": "pc5gf0DcmmXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    X_test_chunk = np.load(f\"{test_path}X_test_chunk_{i}.npy\")\n",
        "    y_test_chunk = np.load(f\"{test_path}y_test_chunk_{i}.npy\")\n",
        "\n",
        "    # Predict\n",
        "    y_pred_chunk = model.predict(X_test_chunk)\n",
        "    y_true_all.append(np.argmax(y_test_chunk, axis=1))  # Convert one-hot to class labels\n",
        "    y_pred_all.append(np.argmax(y_pred_chunk, axis=1))  # Predicted class labels\n",
        "\n",
        "# Combine all chunks\n",
        "y_true_all = np.concatenate(y_true_all)\n",
        "y_pred_all = np.concatenate(y_pred_all)"
      ],
      "metadata": {
        "id": "fkPv1Wm1mmHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all the metrics"
      ],
      "metadata": {
        "id": "6fxCG2oDmxgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save classification report\n",
        "report = classification_report(y_true_all, y_pred_all, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "df_report.to_csv('/content/drive/MyDrive/Model_results/classification_report.csv')\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(7), yticklabels=range(7))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('/content/drive/MyDrive/Model_results/confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Precision, Recall, and F1-Score Calculation\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true_all, y_pred_all, average='weighted')\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Save precision, recall, F1\n",
        "metrics_summary = pd.DataFrame({'Metric': ['Precision', 'Recall', 'F1-Score'], 'Value': [precision, recall, f1]})\n",
        "metrics_summary.to_csv('/content/drive/MyDrive/Model_results/precision_recall_f1.csv', index=False)\n",
        "\n",
        "# AUC-ROC Calculation (for multi-class)\n",
        "# Calculate AUC for each class and average\n",
        "y_true_bin = tf.keras.utils.to_categorical(y_true_all, num_classes=7)\n",
        "y_pred_prob = model.predict(np.concatenate([np.load(f\"{test_path}X_test_chunk_{i}.npy\") for i in range(num_chunks)]))\n",
        "\n",
        "# auc_score = roc_auc_score(y_true_bin, y_pred_prob, average='weighted', multi_class='ovr')\n",
        "# print(f\"Average AUC-ROC: {auc_score:.4f}\")\n",
        "\n",
        "# # Save AUC-ROC score\n",
        "# with open('/content/drive/MyDrive/Model_results/auc_roc.txt', 'w') as f:\n",
        "#     f.write(f\"Average AUC-ROC: {auc_score:.4f}\\n\")"
      ],
      "metadata": {
        "id": "_uU3zHPomxWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}