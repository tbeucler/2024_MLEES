{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example GPU operation\n",
    "with tf.device('/GPU:0'):\n",
    "    matrix1 = tf.constant([[1., 2.], [3., 4.]])\n",
    "    matrix2 = tf.constant([[5., 6.], [7., 8.]])\n",
    "    result = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE IS CLASSIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "else:\n",
    "    print(f\"GPU runtime succesfully selected! We're ready to train our CNNs.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pooch\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd_seed = 42\n",
    "rnd_gen = np.random.default_rng(rnd_seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Loading Tensorboard\n",
    "# Let's clear out the backend and set our random seeds\n",
    "# Consistency makes things easier for labs!\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "#FOR CLASSIC DATA WITHOUT ORDERING\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "base_dir = Path(r'C:\\Users\\bapti\\Downloads\\ML_Project\\Copy_full_fish_dataset')\n",
    "#base_dir = Path(r'C:\\Users\\bapti\\Downloads\\Mask_creation')\n",
    "output_dir = Path(r'C:\\Users\\bapti\\Downloads\\ML_Project\\Data_ready')\n",
    "#output_dir = Path(r'C:\\Users\\bapti\\Downloads\\Mask_data_ready')\n",
    "\n",
    "#For Ubuntu: \n",
    "base_dir = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Copy_full_fish_dataset')\n",
    "output_dir = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready')\n",
    "\n",
    "train_dir = output_dir / 'Train'\n",
    "valid_dir = output_dir / 'Valid'\n",
    "test_dir = output_dir / 'Test'\n",
    "\n",
    "# Ensure output directories exist\n",
    "for path in [train_dir, valid_dir, test_dir]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect all images with metadata\n",
    "data = []\n",
    "test_data = []\n",
    "\n",
    "for class_name in [\"LEFT\", \"RIGHT\"]:\n",
    "    class_dir = base_dir / class_name\n",
    "    for species_name in [\"Grayling\", \"Trout\"]:\n",
    "        species_dir = class_dir / species_name\n",
    "        for date_folder in species_dir.iterdir():\n",
    "            if date_folder.is_dir():\n",
    "                date = date_folder.name\n",
    "                for image_file in date_folder.glob('*.*'):\n",
    "                    # Store metadata with each image path\n",
    "                    item = {\n",
    "                        \"path\": image_file,\n",
    "                        \"class\": class_name,\n",
    "                        \"species\": species_name,\n",
    "                        \"date\": date\n",
    "                    }\n",
    "                    # Assign test data based on specified conditions\n",
    "                    if (species_name == \"Grayling\" and date == \"Day_35\") or \\\n",
    "                       (species_name == \"Trout\" and date == \"Day_146\"):\n",
    "                        test_data.append(item)\n",
    "                    else:\n",
    "                        data.append(item)\n",
    "\n",
    "# # Shuffle data for random splitting\n",
    "# random.seed(123)  # For reproducibility\n",
    "# random.shuffle(data)\n",
    "\n",
    "# # Split remaining data into train (85%) and validation (15%)\n",
    "# total_count = len(data)\n",
    "# train_count = int(0.85 * total_count)\n",
    "\n",
    "# train_data = data[:train_count]\n",
    "# valid_data = data[train_count:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4950 files belonging to 2 classes.\n",
      "Found 874 files belonging to 2 classes.\n",
      "Found 843 files belonging to 2 classes.\n",
      "Class names: ['LEFT', 'RIGHT']\n",
      "Training dataset size: 155\n",
      "Validation dataset size: 28\n",
      "Test dataset size: 27\n",
      "\n",
      " Total number of images in the input dataset: \n",
      " train + valid: 6028 \n",
      " test: 843\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the new directories\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(256, 256),  # Specify your desired image size\n",
    "    batch_size=32,          # Specify your desired batch size\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "train_set.name = 'Training'\n",
    "valid_set.name = 'Validation'\n",
    "test_set.name = 'Test'\n",
    "\n",
    "# Print class names and dataset sizes\n",
    "class_names = train_set.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Training dataset size:\", len(train_set))\n",
    "print(\"Validation dataset size:\", len(valid_set))\n",
    "print(\"Test dataset size:\", len(test_set))\n",
    "\n",
    "# Print number of test images\n",
    "print(\"\\n\",\"Total number of images in the input dataset:\", \"\\n\",\"train + valid:\",len(data), \"\\n\",\"test:\",len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING FOR THE BASELINE MODELS:\n",
    "def preprocessing_function(image, label):\n",
    "    # We're going to hard code the image size we want to use. We can define this\n",
    "    # with a lambda function, but we won't really need to change this and it's\n",
    "    # more trouble than it's worth for us right now :)\n",
    "    # image_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    # Cast the image and label datatypes\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    # Normalize the pixel values. Use a float value in the denominator!\n",
    "    image = image / 255.0\n",
    "\n",
    "    # Resize the image\n",
    "    # image = tf.image.resize(image, ( image_size,  image_size))\n",
    "\n",
    "    # Cast the label to int32 and one-hot encode\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    # Recast label to Float32\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train = train_set.map(preprocessing_function)\n",
    "valid = valid_set.map(preprocessing_function)\n",
    "test = test_set.map(preprocessing_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train\n",
    "validation = valid\n",
    "test = test\n",
    "\n",
    "\n",
    "def get_CNN_logdir():\n",
    "    time = np.datetime64('now').astype(str)[:-3].replace(':', '-')\n",
    "    run_logdir = os.path.join(os.curdir, \"Final_CNN_logs\", f\"Baseline_run_on_classic_data{time}\") # time goes in the fstring\n",
    "    return run_logdir\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Baseline_run_on_classic_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())\n",
    "\n",
    "# Let's clear out the backend and set our random seeds.\n",
    "# Consistency is key :)\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # Convolution 1\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "    # Convolution 2\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 3\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 4\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation=\"softmax\") #Change the last layer from 5 classes to 2 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, 256 , 256, 3))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - accuracy: 0.5175 - loss: 0.7759 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 692ms/step - accuracy: 0.5151 - loss: 0.6931 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 692ms/step - accuracy: 0.5150 - loss: 0.6932 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 690ms/step - accuracy: 0.5173 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 689ms/step - accuracy: 0.5163 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 689ms/step - accuracy: 0.5155 - loss: 0.6933 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 690ms/step - accuracy: 0.5178 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 685ms/step - accuracy: 0.5137 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 684ms/step - accuracy: 0.5149 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 684ms/step - accuracy: 0.5147 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 684ms/step - accuracy: 0.5113 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6923\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# Adjust timestamp to avoid colons\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H_%M_%S\")\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\Baseline_run_on_classic_data{timestamp}\"\n",
    "\n",
    "#For Ubuntu \n",
    "log_dir = Path(f\"Final_CNN_logs/Baseline_run_on_classic_data{timestamp}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "\n",
    "# Define TensorBoard callback with corrected log directory path\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Now run the model training\n",
    "history = model.fit(train, # Training data generator\n",
    "                    epochs=30,\n",
    "                    validation_data=validation, # Validation data generator\n",
    "                    callbacks=[early_stopping_cb,\n",
    "                               checkpoint_cb,\n",
    "                               tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLLOWING IS FOR BASELINE WITH DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Augmented_run_on_classic_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = keras.models.Sequential([\n",
    "    #keras.layers.RandomFlip(), # Flip augmentation removed \n",
    "    keras.layers.RandomRotation(0.08), # Rotation Aumentation\n",
    "    #keras.layers.RandomBrightness([0.2,1.0]),\n",
    "    keras.layers.RandomContrast(0.9),\n",
    "    keras.layers.RandomTranslation(-0.08,0.08),\n",
    "    keras.layers.GaussianNoise( 0.1 , seed =42),\n",
    "    # Convolution 1\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "    # Convolution 2\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 3\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 4\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation=\"softmax\") #Change the last layer from 5 classes to 2 classes\n",
    "\n",
    "    # Copy your previous model's layers here\n",
    "])\n",
    "aug_model.build((None, 256 , 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Set a custom learning rate\n",
    "learning_rate = 0.001  # Try different values: 0.0001, 0.001, 0.005, etc.\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#changed optimizer = \"adam\" to this:\n",
    "aug_model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - accuracy: 0.4854 - loss: 1.0592 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 718ms/step - accuracy: 0.5167 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 813ms/step - accuracy: 0.5137 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 792ms/step - accuracy: 0.5160 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 809ms/step - accuracy: 0.5136 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5137 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 723ms/step - accuracy: 0.5187 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 724ms/step - accuracy: 0.5153 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 806ms/step - accuracy: 0.5167 - loss: 0.6926 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 810ms/step - accuracy: 0.5136 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 724ms/step - accuracy: 0.5137 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5227 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 726ms/step - accuracy: 0.5095 - loss: 0.6933 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 14/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5161 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 15/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 723ms/step - accuracy: 0.5168 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 16/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5138 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 17/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 718ms/step - accuracy: 0.5145 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 18/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5152 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 19/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5166 - loss: 0.6926 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 20/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 788ms/step - accuracy: 0.5167 - loss: 0.6923 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 21/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 725ms/step - accuracy: 0.5165 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 22/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5144 - loss: 0.6928 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 23/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5148 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 24/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 723ms/step - accuracy: 0.5152 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 25/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 720ms/step - accuracy: 0.5168 - loss: 0.6926 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 26/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 722ms/step - accuracy: 0.5151 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6924\n",
      "Epoch 27/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5129 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 28/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5147 - loss: 0.6930 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 29/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 721ms/step - accuracy: 0.5177 - loss: 0.6927 - val_accuracy: 0.5206 - val_loss: 0.6923\n",
      "Epoch 30/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 720ms/step - accuracy: 0.5140 - loss: 0.6929 - val_accuracy: 0.5206 - val_loss: 0.6923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Generate a valid directory path\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\Augmented_run_on_classic_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
    "#For Ubuntu\n",
    "log_dir = Path(f\"Final_CNN_logs/Augmented_run_on_classic_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "#\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Can remove here because made error before with Data augment. model\n",
    "# # Assuming your model currently ends with layers suitable for image output\n",
    "# # Add a Flatten layer to convert the multi-dimensional output to a 1D vector\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# # Add a Dense layer with 5 units (for 5 classes) and softmax activation --> 2 units\n",
    "# # for categorical classification \n",
    "# model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# # Recompile the model with the updated architecture\n",
    "# model.compile(loss=\"categorical_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "history = aug_model.fit(train, # Training data generator\n",
    "                    epochs=30,\n",
    "                    validation_data=validation, # Validation data generator\n",
    "                    callbacks=[early_stopping_cb,\n",
    "                               checkpoint_cb,\n",
    "                               tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50_epoch_models', 'Augmented_run_on_classic_data.h5', 'Augmented_run_on_classic_data.keras', 'Augmented_run_on_mask_data.h5', 'Baseline_run_on_classic_data.h5', 'Baseline_run_on_classic_data.keras', 'Baseline_run_on_mask_data.h5', 'CNN_logs', 'CNN_ResNet_classic_data.h5', 'CNN_ResNet_mask_data.h5', 'Copy_full_fish_dataset', 'Data_ready', 'Final_CNN_logs', 'images', 'Mask_data_ready', 'mp4_to_mp3.py', 'Obsolete_Data_to_be_hand_classified', 'Obsolete_DAY0_fish_dataset', 'Obsolete_Fish_Mask_Output', 'ResNet.ipynb', 'S5_1_CNNs_Brown_trout_data.ipynb', 'Segment_Anything_SAM.py', 'test_ZF_Net.py']\n",
      "/mnt/c/Users/bapti/Downloads/ML_Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - accuracy: 0.4846 - loss: 0.6947\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 204ms/step - accuracy: 0.4776 - loss: 0.6951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6942036747932434, 0.4911032021045685]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "model.save('Baseline_run_on_classic_data.h5')\n",
    "aug_model.save('Augmented_run_on_classic_data.h5')\n",
    "\n",
    "# Let's load the models!\n",
    "non_aug_model = keras.models.load_model(r'Baseline_run_on_classic_data.h5')\n",
    "aug_model = keras.models.load_model(r'Augmented_run_on_classic_data.h5')\n",
    "\n",
    "# And test them on the testing dataset\n",
    "non_aug_model.evaluate(test)\n",
    "aug_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING FOR THE RESNET:\n",
    "\n",
    "def preprocessing_function(image, label):\n",
    "    # We're going to hard code the image size we want to use. We can define this\n",
    "    # with a lambda function, but we won't really need to change this and it's\n",
    "    # more trouble than it's worth for us right now :)\n",
    "    # image_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    # Cast the image and label datatypes\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    # Resize the image to (224, 224) for ResNet50\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    \n",
    "    # Normalize the pixel values. Use a float value in the denominator!\n",
    "    image = image / 255.0\n",
    "\n",
    "    # Resize the image\n",
    "    # image = tf.image.resize(image, ( image_size,  image_size))\n",
    "\n",
    "    # Cast the label to int32 and one-hot encode\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    # Recast label to Float32\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train = train_set.map(preprocessing_function)\n",
    "valid = valid_set.map(preprocessing_function)\n",
    "test = test_set.map(preprocessing_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (32, 224, 224, 3) Labels: (32, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 19:06:30.056657: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for images, labels in train.take(1):\n",
    "  print(f'Images shape: {images.numpy().shape} Labels: {labels.numpy().shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baptiste07/miniconda3/envs/GPU/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 2s/step - accuracy: 0.8292 - loss: 0.3311 - val_accuracy: 0.5206 - val_loss: 0.6938\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 2s/step - accuracy: 0.9963 - loss: 0.0089 - val_accuracy: 0.4794 - val_loss: 0.9048\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.8464e-04 - val_accuracy: 0.4794 - val_loss: 0.7813\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.6556e-05 - val_accuracy: 0.4805 - val_loss: 0.7347\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.6792e-05 - val_accuracy: 0.8432 - val_loss: 0.2778\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.0947e-05 - val_accuracy: 0.9989 - val_loss: 0.0074\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.9437e-05 - val_accuracy: 1.0000 - val_loss: 5.4983e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.4587e-05 - val_accuracy: 1.0000 - val_loss: 6.7496e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.7671e-05 - val_accuracy: 1.0000 - val_loss: 6.7226e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.0815e-05 - val_accuracy: 1.0000 - val_loss: 6.7242e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.1428e-06 - val_accuracy: 1.0000 - val_loss: 6.6726e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 6.9520e-06 - val_accuracy: 1.0000 - val_loss: 6.7089e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.0970e-06 - val_accuracy: 1.0000 - val_loss: 6.5580e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.6222e-06 - val_accuracy: 1.0000 - val_loss: 6.4078e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.9029e-06 - val_accuracy: 1.0000 - val_loss: 6.1605e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.0526e-06 - val_accuracy: 1.0000 - val_loss: 5.5441e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.7477e-06 - val_accuracy: 1.0000 - val_loss: 5.5329e-04\n"
     ]
    }
   ],
   "source": [
    "###CNN_RESNET_ON_CLASSIC_DATA ALREADY SAVED\n",
    "\n",
    "train = train\n",
    "validation = valid\n",
    "test = test\n",
    "\n",
    "\n",
    "def get_CNN_logdir():\n",
    "    time = np.datetime64('now').astype(str)[:-3].replace(':', '-')\n",
    "    run_logdir = os.path.join(os.curdir, \"Final_CNN_logs\", f\"ResNet_run_on_classic_data{time}\") # time goes in the fstring\n",
    "    return run_logdir\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"CNN_ResNet_classic_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())\n",
    "\n",
    "# Let's clear out the backend and set our random seeds.\n",
    "# Consistency is key :)\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# Load ResNet50 with no top layers (without classification layers)\n",
    "base_model = keras.applications.resnet50.ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze the base model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = False\n",
    "# base_model.trainable = False\n",
    "\n",
    "#inputs = train.map(lambda image, label: (keras.applications.resnet50.preprocess_input(image * 255), label))\n",
    "\n",
    "inputs = train.map(lambda image, label: (image, label))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.01) \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]) \n",
    "\n",
    "import datetime\n",
    "\n",
    "# Generate a valid directory path\n",
    "\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\ResNet_run_on_classic_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
    "\n",
    "#For Ubuntu\n",
    "log_dir = Path(f\"Final_CNN_logs/ResNet_run_on_classic_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "history = model.fit(train, epochs=30, validation_data=valid, callbacks=[early_stopping_cb,checkpoint_cb,tensorboard_cb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('CNN_ResNet_classic_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4947 files belonging to 2 classes.\n",
      "Found 874 files belonging to 2 classes.\n",
      "Found 822 files belonging to 2 classes.\n",
      "Class names: ['LEFT', 'RIGHT']\n",
      "Training dataset size: 155\n",
      "Validation dataset size: 28\n",
      "Test dataset size: 26\n",
      "\n",
      " Total number of images in the input dataset: \n",
      " train + valid: 5821 \n",
      " test: 822\n"
     ]
    }
   ],
   "source": [
    "#CHANGE DATA TO MASK DATA\n",
    "\n",
    "#FOR CLASSIC DATA WITHOUT ORDERING\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "#base_dir = Path(r'C:\\Users\\bapti\\Downloads\\ML_Project\\Copy_full_fish_dataset')\n",
    "base_dir = Path(r'C:\\Users\\bapti\\Downloads\\Mask_creation')\n",
    "#output_dir = Path(r'C:\\Users\\bapti\\Downloads\\ML_Project\\Data_ready')\n",
    "output_dir = Path(r'C:\\Users\\bapti\\Downloads\\Mask_data_ready')\n",
    "#For Ubuntu: \n",
    "base_dir = Path('/mnt/c/Users/bapti/Downloads/Mask_creation')\n",
    "output_dir = Path('/mnt/c/Users/bapti/Downloads/Mask_data_ready')\n",
    "\n",
    "train_dir = output_dir / 'Train'\n",
    "valid_dir = output_dir / 'Valid'\n",
    "test_dir = output_dir / 'Test'\n",
    "\n",
    "# Ensure output directories exist\n",
    "for path in [train_dir, valid_dir, test_dir]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect all images with metadata\n",
    "data = []\n",
    "test_data = []\n",
    "\n",
    "for class_name in [\"LEFT\", \"RIGHT\"]:\n",
    "    class_dir = base_dir / class_name\n",
    "    for species_name in [\"Grayling\", \"Trout\"]:\n",
    "        species_dir = class_dir / species_name\n",
    "        for date_folder in species_dir.iterdir():\n",
    "            if date_folder.is_dir():\n",
    "                date = date_folder.name\n",
    "                for image_file in date_folder.glob('*.*'):\n",
    "                    # Store metadata with each image path\n",
    "                    item = {\n",
    "                        \"path\": image_file,\n",
    "                        \"class\": class_name,\n",
    "                        \"species\": species_name,\n",
    "                        \"date\": date\n",
    "                    }\n",
    "                    # Assign test data based on specified conditions\n",
    "                    if (species_name == \"Grayling\" and date == \"Day_35\") or \\\n",
    "                       (species_name == \"Trout\" and date == \"Day_146\"):\n",
    "                        test_data.append(item)\n",
    "                    else:\n",
    "                        data.append(item)\n",
    "\n",
    "# Shuffle data for random splitting\n",
    "random.seed(123)  # For reproducibility\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split remaining data into train (85%) and validation (15%)\n",
    "total_count = len(data)\n",
    "train_count = int(0.85 * total_count)\n",
    "\n",
    "train_data = data[:train_count]\n",
    "valid_data = data[train_count:]\n",
    "\n",
    "# Load the datasets from the new directories\n",
    "train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(256, 256),  # Specify your desired image size\n",
    "    batch_size=32,          # Specify your desired batch size\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "valid_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "train_set.name = 'Training'\n",
    "valid_set.name = 'Validation'\n",
    "test_set.name = 'Test'\n",
    "\n",
    "# Print class names and dataset sizes\n",
    "class_names = train_set.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Training dataset size:\", len(train_set))\n",
    "print(\"Validation dataset size:\", len(valid_set))\n",
    "print(\"Test dataset size:\", len(test_set))\n",
    "\n",
    "# Print number of test images\n",
    "print(\"\\n\",\"Total number of images in the input dataset:\", \"\\n\",\"train + valid:\",len(data), \"\\n\",\"test:\",len(test_data))\n",
    "\n",
    "#batch size set to 1 instead of 32 because of low data\n",
    "\n",
    "def preprocessing_function(image, label):\n",
    "    # We're going to hard code the image size we want to use. We can define this\n",
    "    # with a lambda function, but we won't really need to change this and it's\n",
    "    # more trouble than it's worth for us right now :)\n",
    "    # image_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    # Cast the image and label datatypes\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    # Resize the image to (224, 224) for ResNet50\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    \n",
    "    # Normalize the pixel values. Use a float value in the denominator!\n",
    "    image = image / 255.0\n",
    "\n",
    "    # Resize the image\n",
    "    # image = tf.image.resize(image, ( image_size,  image_size))\n",
    "\n",
    "    # Cast the label to int32 and one-hot encode\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    # Recast label to Float32\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train = train_set.map(preprocessing_function)\n",
    "valid = valid_set.map(preprocessing_function)\n",
    "test = test_set.map(preprocessing_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEY\n"
     ]
    }
   ],
   "source": [
    "###CNN_RESNET_ON_MASK_DATA ALREADY SAVED\n",
    "\n",
    "train = train\n",
    "validation = valid\n",
    "test = test\n",
    "\n",
    "\n",
    "def get_CNN_logdir():\n",
    "    time = np.datetime64('now').astype(str)[:-3].replace(':', '-')\n",
    "    run_logdir = os.path.join(os.curdir, \"Final_CNN_logs\", f\"ResNet_run_on_mask_data{time}\") # time goes in the fstring\n",
    "    return run_logdir\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=40)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"CNN_ResNet_mask_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())\n",
    "\n",
    "# Let's clear out the backend and set our random seeds.\n",
    "# Consistency is key :)\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n",
    "print(\"HEY\")\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# Load ResNet50 with no top layers (without classification layers)\n",
    "base_model = keras.applications.resnet50.ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze the base model\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = False\n",
    "# base_model.trainable = False\n",
    "\n",
    "#inputs = train.map(lambda image, label: (keras.applications.resnet50.preprocess_input(image * 255), label))\n",
    "\n",
    "inputs = train.map(lambda image, label: (image, label))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.01) \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]) \n",
    "\n",
    "import datetime\n",
    "\n",
    "# Generate a valid directory path\n",
    "\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\ResNet_run_on_mask_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
    "#For Ubuntu\n",
    "log_dir = Path(f\"Final_CNN_logs/ResNet_run_on_mask_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "history = model.fit(train, epochs=30, validation_data=valid, callbacks=[early_stopping_cb,checkpoint_cb,tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('CNN_ResNet_mask_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Has not worked, plot the expected side of the fish:\n",
    "\n",
    "# inputs = test.map(lambda image, label: (keras.applications.resnet50.preprocess_input(image * 255), label))\n",
    "# # inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
    "# Y_proba = model.predict(inputs)\n",
    "\n",
    "# # Display the probabilities for LEFT and RIGHT\n",
    "# for image_index in range(min(25, len(Y_proba))):\n",
    "#     print(f\"Image #{image_index}\")\n",
    "    \n",
    "#     plt.figure(figsize=(5, 5))\n",
    "    \n",
    "#     # Show the image (use images.numpy() to convert the tensor to a numpy array)\n",
    "#     plt.imshow(images[image_index].numpy())  # Assuming 'images' is a batch of images\n",
    "#     plt.axis('off')  # Hide axis\n",
    "    \n",
    "#     # Display the probabilities for LEFT and RIGHT\n",
    "#     left_prob = Y_proba[image_index][0] * 100  # Probability for LEFT class\n",
    "#     right_prob = Y_proba[image_index][1] * 100  # Probability for RIGHT class\n",
    "#     plt.title(f\"Image #{image_index}\\nLEFT: {left_prob:.2f}% | RIGHT: {right_prob:.2f}%\", fontsize=12)\n",
    "    \n",
    "#     # Show the image with the prediction\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50_epoch_models', 'Augmented_run_on_classic_data.h5', 'Augmented_run_on_classic_data.keras', 'Augmented_run_on_mask_data.h5', 'Baseline_run_on_classic_data.h5', 'Baseline_run_on_classic_data.keras', 'Baseline_run_on_mask_data.h5', 'Baseline_run_on_mask_data.keras', 'CNN_logs', 'CNN_ResNet_classic_data.h5', 'CNN_ResNet_classic_data.keras', 'CNN_ResNet_mask_data.h5', 'CNN_ResNet_mask_data.keras', 'Copy_full_fish_dataset', 'Data_ready', 'Final_CNN_logs', 'images', 'Mask_data_ready', 'mp4_to_mp3.py', 'Obsolete_Data_to_be_hand_classified', 'Obsolete_DAY0_fish_dataset', 'Obsolete_Fish_Mask_Output', 'ResNet.ipynb', 'S5_1_CNNs_Brown_trout_data.ipynb', 'Segment_Anything_SAM.py', 'test_ZF_Net.py', 'tmp4zvhfqdx']\n",
      "/mnt/c/Users/bapti/Downloads/ML_Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "# # model.save('CNN_ResNet.h5')\n",
    "# #aug_model = keras.models.load_model('path/to/your/directory/CNN_augmented.h5')\n",
    "# # Let's load the models!\n",
    "######## ALREADY SAVED\n",
    "# classic_model = keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# mask_model = keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "\n",
    "# And test them on the testing dataset\n",
    "# classic_model.evaluate(test)\n",
    "# mask_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # # TO SEE RESULTS:\n",
    "# %tensorboard --logdir=./Final_CNN_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 780ms/step - accuracy: 0.7679 - loss: 0.4824 - val_accuracy: 0.9748 - val_loss: 0.0747\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 773ms/step - accuracy: 0.9915 - loss: 0.0292 - val_accuracy: 0.9897 - val_loss: 0.0292\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 698ms/step - accuracy: 0.9957 - loss: 0.0110 - val_accuracy: 0.9943 - val_loss: 0.0303\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 776ms/step - accuracy: 0.9945 - loss: 0.0142 - val_accuracy: 0.9920 - val_loss: 0.0247\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 848ms/step - accuracy: 0.9906 - loss: 0.0277 - val_accuracy: 0.9908 - val_loss: 0.0296\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 626ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9908 - val_loss: 0.0416\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 639ms/step - accuracy: 0.9917 - loss: 0.0335 - val_accuracy: 0.9943 - val_loss: 0.0280\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 678ms/step - accuracy: 0.9996 - loss: 0.0021 - val_accuracy: 0.9966 - val_loss: 0.0301\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 697ms/step - accuracy: 0.9988 - loss: 0.0032 - val_accuracy: 0.9897 - val_loss: 0.0460\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 696ms/step - accuracy: 0.9972 - loss: 0.0085 - val_accuracy: 0.9954 - val_loss: 0.0264\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 695ms/step - accuracy: 1.0000 - loss: 6.9873e-04 - val_accuracy: 0.9943 - val_loss: 0.0280\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 695ms/step - accuracy: 1.0000 - loss: 1.0068e-04 - val_accuracy: 0.9954 - val_loss: 0.0310\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 695ms/step - accuracy: 1.0000 - loss: 9.6923e-06 - val_accuracy: 0.9954 - val_loss: 0.0320\n",
      "Epoch 14/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 698ms/step - accuracy: 1.0000 - loss: 7.2792e-06 - val_accuracy: 0.9954 - val_loss: 0.0329\n",
      "Epoch 15/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 697ms/step - accuracy: 1.0000 - loss: 5.9643e-06 - val_accuracy: 0.9954 - val_loss: 0.0330\n",
      "Epoch 16/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 696ms/step - accuracy: 1.0000 - loss: 5.3678e-06 - val_accuracy: 0.9954 - val_loss: 0.0336\n",
      "Epoch 17/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 885ms/step - accuracy: 1.0000 - loss: 3.7098e-06 - val_accuracy: 0.9954 - val_loss: 0.0346\n",
      "Epoch 18/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 687ms/step - accuracy: 1.0000 - loss: 2.2200e-06 - val_accuracy: 0.9954 - val_loss: 0.0352\n",
      "Epoch 19/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 688ms/step - accuracy: 1.0000 - loss: 1.1588e-06 - val_accuracy: 0.9954 - val_loss: 0.0367\n",
      "Epoch 20/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 688ms/step - accuracy: 1.0000 - loss: 1.0720e-06 - val_accuracy: 0.9954 - val_loss: 0.0387\n",
      "Epoch 21/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 684ms/step - accuracy: 1.0000 - loss: 1.2339e-06 - val_accuracy: 0.9954 - val_loss: 0.0391\n",
      "Epoch 22/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 688ms/step - accuracy: 1.0000 - loss: 7.8784e-07 - val_accuracy: 0.9954 - val_loss: 0.0406\n",
      "Epoch 23/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 686ms/step - accuracy: 1.0000 - loss: 7.4677e-07 - val_accuracy: 0.9954 - val_loss: 0.0409\n",
      "Epoch 24/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 685ms/step - accuracy: 1.0000 - loss: 3.2833e-07 - val_accuracy: 0.9954 - val_loss: 0.0411\n",
      "Epoch 25/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 686ms/step - accuracy: 1.0000 - loss: 2.4149e-07 - val_accuracy: 0.9954 - val_loss: 0.0406\n",
      "Epoch 26/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 687ms/step - accuracy: 1.0000 - loss: 3.4405e-07 - val_accuracy: 0.9954 - val_loss: 0.0417\n",
      "Epoch 27/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 685ms/step - accuracy: 1.0000 - loss: 1.9399e-07 - val_accuracy: 0.9954 - val_loss: 0.0411\n",
      "Epoch 28/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 687ms/step - accuracy: 1.0000 - loss: 2.7390e-07 - val_accuracy: 0.9954 - val_loss: 0.0407\n",
      "Epoch 29/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 686ms/step - accuracy: 1.0000 - loss: 1.1564e-07 - val_accuracy: 0.9954 - val_loss: 0.0413\n",
      "Epoch 30/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 685ms/step - accuracy: 1.0000 - loss: 2.0134e-07 - val_accuracy: 0.9954 - val_loss: 0.0413\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# BASELINE RUN ON MASK DATA:\n",
    "#\n",
    "# \n",
    " #PREPROCESSING FOR THE BASELINE MODELS:\n",
    "def preprocessing_function(image, label):\n",
    "    # We're going to hard code the image size we want to use. We can define this\n",
    "    # with a lambda function, but we won't really need to change this and it's\n",
    "    # more trouble than it's worth for us right now :)\n",
    "    # image_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    # Cast the image and label datatypes\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    # Normalize the pixel values. Use a float value in the denominator!\n",
    "    image = image / 255.0\n",
    "\n",
    "    # Resize the image\n",
    "    # image = tf.image.resize(image, ( image_size,  image_size))\n",
    "\n",
    "    # Cast the label to int32 and one-hot encode\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    # Recast label to Float32\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train = train_set.map(preprocessing_function)\n",
    "valid = valid_set.map(preprocessing_function)\n",
    "test = test_set.map(preprocessing_function)\n",
    "\n",
    "train = train\n",
    "validation = valid\n",
    "test = test\n",
    "\n",
    "\n",
    "def get_CNN_logdir():\n",
    "    time = np.datetime64('now').astype(str)[:-3].replace(':', '-')\n",
    "    run_logdir = os.path.join(os.curdir, \"Final_CNN_logs\", f\"Baseline_run_on_mask_data{time}\") # time goes in the fstring\n",
    "    return run_logdir\n",
    "\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=40)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Baseline_run_on_mask_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())\n",
    "\n",
    "# Let's clear out the backend and set our random seeds.\n",
    "# Consistency is key :)\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # Convolution 1\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "    # Convolution 2\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 3\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 4\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation=\"softmax\") #Change the last layer from 5 classes to 2 classes\n",
    "])\n",
    "\n",
    "model.build((None, 256 , 256, 3))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Adjust timestamp to avoid colons\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H_%M_%S\")\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\Baseline_run_on_mask_data{timestamp}\"\n",
    "#For Ubuntu\n",
    "log_dir = Path(f\"Final_CNN_logs/Baseline_run_on_mask_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "\n",
    "# Define TensorBoard callback with corrected log directory path\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Now run the model training\n",
    "history = model.fit(train, # Training data generator\n",
    "                    epochs=30,\n",
    "                    validation_data=validation, # Validation data generator\n",
    "                    callbacks=[early_stopping_cb,\n",
    "                               checkpoint_cb,\n",
    "                               tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 752ms/step - accuracy: 0.7186 - loss: 0.5261 - val_accuracy: 0.9897 - val_loss: 0.0472\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 686ms/step - accuracy: 0.9700 - loss: 0.1072 - val_accuracy: 0.9886 - val_loss: 0.0870\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 759ms/step - accuracy: 0.9817 - loss: 0.0681 - val_accuracy: 0.9943 - val_loss: 0.0236\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 756ms/step - accuracy: 0.9883 - loss: 0.0425 - val_accuracy: 0.9920 - val_loss: 0.0221\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 756ms/step - accuracy: 0.9895 - loss: 0.0317 - val_accuracy: 0.9989 - val_loss: 0.0149\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 761ms/step - accuracy: 0.9917 - loss: 0.0288 - val_accuracy: 0.9989 - val_loss: 0.0031\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 680ms/step - accuracy: 0.9943 - loss: 0.0231 - val_accuracy: 0.9954 - val_loss: 0.0281\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 674ms/step - accuracy: 0.9806 - loss: 0.0843 - val_accuracy: 0.9966 - val_loss: 0.0121\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 675ms/step - accuracy: 0.9952 - loss: 0.0258 - val_accuracy: 0.9954 - val_loss: 0.0119\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 758ms/step - accuracy: 0.9953 - loss: 0.0132 - val_accuracy: 0.9989 - val_loss: 0.0024\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 678ms/step - accuracy: 0.9939 - loss: 0.0243 - val_accuracy: 0.9931 - val_loss: 0.0276\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 678ms/step - accuracy: 0.9973 - loss: 0.0133 - val_accuracy: 0.9966 - val_loss: 0.0070\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 675ms/step - accuracy: 0.9924 - loss: 0.0277 - val_accuracy: 0.9954 - val_loss: 0.0198\n",
      "Epoch 14/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 677ms/step - accuracy: 0.9955 - loss: 0.0152 - val_accuracy: 0.9966 - val_loss: 0.0216\n",
      "Epoch 15/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 676ms/step - accuracy: 0.9944 - loss: 0.0322 - val_accuracy: 0.9954 - val_loss: 0.0124\n",
      "Epoch 16/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 676ms/step - accuracy: 0.9958 - loss: 0.0167 - val_accuracy: 0.9977 - val_loss: 0.0055\n",
      "Epoch 17/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 677ms/step - accuracy: 0.9937 - loss: 0.0312 - val_accuracy: 0.9989 - val_loss: 0.0028\n",
      "Epoch 18/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 677ms/step - accuracy: 0.9984 - loss: 0.0086 - val_accuracy: 0.9977 - val_loss: 0.0051\n",
      "Epoch 19/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 677ms/step - accuracy: 0.9962 - loss: 0.0145 - val_accuracy: 0.9977 - val_loss: 0.0181\n",
      "Epoch 20/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 676ms/step - accuracy: 0.9940 - loss: 0.0212 - val_accuracy: 0.9931 - val_loss: 0.0269\n"
     ]
    }
   ],
   "source": [
    "# AUGMENTED RUN ON MASK DATA\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(rnd_seed)\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Augmented_run_on_mask_data.keras\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='val_loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_CNN_logdir())\n",
    "\n",
    "aug_model = keras.models.Sequential([\n",
    "    #keras.layers.RandomFlip(), # Flip augmentation removed \n",
    "    keras.layers.RandomRotation(0.08), # Rotation Aumentation\n",
    "    #keras.layers.RandomBrightness([0.2,1.0]),\n",
    "    keras.layers.RandomContrast(0.9),\n",
    "    keras.layers.RandomTranslation(-0.08,0.08),\n",
    "    # Convolution 1\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "    # Convolution 2\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=5, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 3\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    # Convolution 4\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation=\"softmax\") #Change the last layer from 5 classes to 2 classes\n",
    "\n",
    "    # Copy your previous model's layers here\n",
    "])\n",
    "aug_model.build((None, 256 , 256, 3))\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set a custom learning rate\n",
    "learning_rate = 0.001  # Try different values: 0.0001, 0.001, 0.005, etc.\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "#changed optimizer = \"adam\" to this:\n",
    "aug_model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Generate a valid directory path\n",
    "log_dir = f\".\\\\Final_CNN_logs\\\\Augmented_run_on_mask_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
    "#For Ubuntu\n",
    "log_dir = Path(f\"Final_CNN_logs/Augmented_run_on_mask_data{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\")\n",
    "log_dir_str = log_dir.as_posix()\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Can remove here because made error before with Data augment. model\n",
    "# # Assuming your model currently ends with layers suitable for image output\n",
    "# # Add a Flatten layer to convert the multi-dimensional output to a 1D vector\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# # Add a Dense layer with 5 units (for 5 classes) and softmax activation --> 2 units\n",
    "# # for categorical classification \n",
    "# model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# # Recompile the model with the updated architecture\n",
    "# model.compile(loss=\"categorical_crossentropy\",\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "history = aug_model.fit(train, # Training data generator\n",
    "                    epochs=30,\n",
    "                    validation_data=validation, # Validation data generator\n",
    "                    callbacks=[early_stopping_cb,\n",
    "                               checkpoint_cb,\n",
    "                               tensorboard_cb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50_epoch_models', 'Augmented_run_on_classic_data.h5', 'Augmented_run_on_classic_data.keras', 'Augmented_run_on_mask_data.h5', 'Augmented_run_on_mask_data.keras', 'Baseline_run_on_classic_data.h5', 'Baseline_run_on_classic_data.keras', 'Baseline_run_on_mask_data.h5', 'Baseline_run_on_mask_data.keras', 'CNN_logs', 'CNN_ResNet_classic_data.h5', 'CNN_ResNet_classic_data.keras', 'CNN_ResNet_mask_data.h5', 'CNN_ResNet_mask_data.keras', 'Copy_full_fish_dataset', 'Data_ready', 'Final_CNN_logs', 'images', 'Mask_data_ready', 'mp4_to_mp3.py', 'Obsolete_Data_to_be_hand_classified', 'Obsolete_DAY0_fish_dataset', 'Obsolete_Fish_Mask_Output', 'ResNet.ipynb', 'S5_1_CNNs_Brown_trout_data.ipynb', 'Segment_Anything_SAM.py', 'test_ZF_Net.py', 'tmp4zvhfqdx']\n",
      "/mnt/c/Users/bapti/Downloads/ML_Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - accuracy: 0.9970 - loss: 0.0269\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.9622 - loss: 0.1312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10713861882686615, 0.9659367203712463]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "model.save('Baseline_run_on_mask_data.h5')\n",
    "aug_model.save('Augmented_run_on_mask_data.h5')\n",
    "#aug_model = keras.models.load_model('path/to/your/directory/CNN_augmented.h5')\n",
    "# Let's load the models!\n",
    "non_aug_model = keras.models.load_model(r'Baseline_run_on_mask_data.h5')\n",
    "aug_model = keras.models.load_model(r'Augmented_run_on_mask_data.h5')\n",
    "\n",
    "# And test them on the testing dataset\n",
    "non_aug_model.evaluate(test)\n",
    "aug_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 843 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 222ms/step - accuracy: 0.5136 - loss: 0.6725\n",
      "Augmented Model - Test Loss: 0.6729666590690613\n",
      "Augmented Model - Test Accuracy: 0.5072992444038391\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - accuracy: 0.8398 - loss: 40.0959\n",
      "Non-Augmented Model - Test Loss: 124.92329406738281\n",
      "Non-Augmented Model - Test Accuracy: 0.5088967680931091\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "#\"Test the model on new data:\n",
    "\n",
    "##Test on \"mask data\" (if trained on classic data)\n",
    "#data_to_test = r\"C:\\Users\\bapti\\Downloads\\ML_Project\\Data_ready\\Test\"\n",
    "##Test on \"classic data\" (if trained on mask data)\n",
    "\n",
    "path_data_to_test = r\"C:\\Users\\bapti\\Downloads\\Mask_data_ready\\Test\"\n",
    "#For UBUNTU:\n",
    "#On mask\n",
    "path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/Mask_data_ready/Test')\n",
    "#On classic\n",
    "path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready/Test')\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    path_data_to_test,\n",
    "    image_size=(256, 256), #CHANGE RESIER TO SIZE ACCEPTED BY MODEL\n",
    "    batch_size=16,\n",
    "    shuffle=False  # Do not shuffle if the dataset order matters\n",
    ")\n",
    "def one_hot_encode_labels(images, labels):\n",
    "    labels = tf.one_hot(labels, depth=2)\n",
    "    return images, labels\n",
    "test_dataset = test_dataset.map(one_hot_encode_labels)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "# #Which model?\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "model_ready = keras.models.load_model(r'Baseline_run_on_mask_data.h5')\n",
    "\n",
    "# Evaluate the augmented model\n",
    "aug_results = model_ready.evaluate(test) \n",
    "print(\"Augmented Model - Test Loss:\", aug_results[0])\n",
    "print(\"Augmented Model - Test Accuracy:\", aug_results[1])\n",
    "\n",
    "# Evaluate the non-augmented model\n",
    "non_aug_results = model.evaluate(test_dataset)\n",
    "print(\"Non-Augmented Model - Test Loss:\", non_aug_results[0])\n",
    "print(\"Non-Augmented Model - Test Accuracy:\", non_aug_results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SALIENCY MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/home/baptiste07/miniconda3/envs/GPU/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: input_layer. Received: the structure of inputs=['*']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your model\n",
    "# model_ready = load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "model_ready = keras.models.load_model(r'Baseline_run_on_mask_data.h5')\n",
    "# Prepare the image\n",
    "path_data_to_test = r\"C:\\Users\\bapti\\Downloads\\Mask_data_ready\\Test\\LEFT\\Trout\\IMG_7946_processed.jpg\"\n",
    "\n",
    "#For UBUNTU:\n",
    "path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/Mask_data_ready/Test/LEFT/Trout/IMG_7946_processed.jpg')\n",
    "# path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready/Test/LEFT/Trout/IMG_7946.jpg')\n",
    "print(model_ready.input_shape)\n",
    "\n",
    "img = load_img(path_data_to_test, target_size=(256, 256)) # CHANGE THE RESIZE FOR THE CORECT MODEL + HAVE THE RIGHT PREPROCESSING FUNCTION\n",
    "x = img_to_array(img)  # Convert to numpy array\n",
    "x = x.reshape((1,) + x.shape)  # Add batch dimension\n",
    "x = x / 255.0  # Normalize image data if your model expects normalized inputs\n",
    "\n",
    "# Convert the last activation layer to linear\n",
    "model_ready.layers[-1].activation = None  # For TensorFlow 2.x compatibility\n",
    "\n",
    "# Define the class index you want to visualize\n",
    "class_index = 0  # Change this to match your target class index\n",
    "score = CategoricalScore([class_index])\n",
    "\n",
    "# Create Saliency object\n",
    "saliency = Saliency(model_ready, clone=False)\n",
    "\n",
    "# Generate saliency map\n",
    "saliency_map = saliency(score, x, smooth_samples=20, smooth_noise=0.2)\n",
    "saliency_map = normalize(saliency_map)  # Normalize values for visualization\n",
    "\n",
    "# Plot saliency map\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(saliency_map[0], cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 25600, but received input with shape (1, 20736)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=['tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)']\n  • training=False\n  • mask=['None']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m saliency \u001b[38;5;241m=\u001b[39m Saliency(model_ready, clone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Generate saliency map\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m saliency_map \u001b[38;5;241m=\u001b[39m \u001b[43msaliency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m saliency_map \u001b[38;5;241m=\u001b[39m normalize(saliency_map)  \u001b[38;5;66;03m# Normalize values for visualization\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Slightly amplify the saliency map\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GPU/lib/python3.12/site-packages/tf_keras_vis/saliency.py:95\u001b[0m, in \u001b[0;36mSaliency.__call__\u001b[0;34m(self, score, seed_input, smooth_samples, smooth_noise, keepdims, gradient_modifier, training, normalize_map, unconnected_gradients)\u001b[0m\n\u001b[1;32m     93\u001b[0m total \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mzeros_like(X[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m seed_inputs)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(smooth_samples):\n\u001b[0;32m---> 95\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_modifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     total \u001b[38;5;241m=\u001b[39m (total \u001b[38;5;241m+\u001b[39m g \u001b[38;5;28;01mfor\u001b[39;00m total, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(total, grads))\n\u001b[1;32m     98\u001b[0m grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;241m/\u001b[39m smooth_samples \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m total]\n",
      "File \u001b[0;32m~/miniconda3/envs/GPU/lib/python3.12/site-packages/tf_keras_vis/saliency.py:115\u001b[0m, in \u001b[0;36mSaliency._get_gradients\u001b[0;34m(self, seed_inputs, scores, gradient_modifier, training, unconnected_gradients)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(watch_accessed_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    114\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(seed_inputs)\n\u001b[0;32m--> 115\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m listify(outputs)\n\u001b[1;32m    117\u001b[0m     score_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_scores(outputs, scores)\n",
      "File \u001b[0;32m~/miniconda3/envs/GPU/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/GPU/lib/python3.12/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 25600, but received input with shape (1, 20736)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=['tf.Tensor(shape=(1, 224, 224, 3), dtype=float32)']\n  • training=False\n  • mask=['None']"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# # ALREADY LOADED ABOVE \n",
    "# #model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# # model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# # model_ready =  keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "# model_ready = keras.models.load_model(r'Baseline_run_on_mask_data.h5')\n",
    "# model_ready = keras.models.load_model(r'Augmented_run_on_mask_data.h5')\n",
    "# model_ready = keras.models.load_model(r'Augmented_run_on_classic_data.h5')\n",
    "# # print(\"model_ready.summary()\")\n",
    "# # print(model_ready.summary())\n",
    "# # print(\"model_ready.summary()\")\n",
    "# # prediction = model_ready.predict(x)\n",
    "# # print(\"Predicted class probabilities:\", prediction)\n",
    "\n",
    "# Prepare the image\n",
    "path_data_to_test = r\"C:\\Users\\bapti\\Downloads\\Mask_data_ready\\Test\\LEFT\\Trout\\IMG_7946_processed.jpg\"\n",
    "\n",
    "#For UBUNTU:\n",
    "#Left mask data\n",
    "# path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/Mask_data_ready/Test/LEFT/Trout/IMG_7946_processed.jpg')\n",
    "# #Right mask data\n",
    "# path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/Mask_data_ready/Test/RIGHT/Trout/IMG_8380_processed.jpg')\n",
    "# #path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready/Test/LEFT/Trout/IMG_7946.jpg')\n",
    "# #Left classic data\n",
    "path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready/Test/LEFT/Trout/IMG_7946.jpg')\n",
    "# #Right classic data\n",
    "# path_data_to_test = Path('/mnt/c/Users/bapti/Downloads/ML_Project/Data_ready/Test/RIGHT/Trout/IMG_8380.jpg')\n",
    "\n",
    "img = load_img(path_data_to_test, target_size=(224, 224))\n",
    "x = img_to_array(img)  # Convert to numpy array\n",
    "x = x.reshape((1,) + x.shape)  # Add batch dimension\n",
    "x = x / 255.0  # Normalize image data if your model expects normalized inputs\n",
    "\n",
    "# Convert the last activation layer to linear\n",
    "model_ready.layers[-1].activation = None  # For TensorFlow 2.x compatibility\n",
    "\n",
    "# Define the class index you want to visualize\n",
    "class_index = 0  # Change this to match your target class index\n",
    "score = CategoricalScore([class_index])\n",
    "\n",
    "# Create Saliency object\n",
    "saliency = Saliency(model_ready, clone=False)\n",
    "\n",
    "# Generate saliency map\n",
    "saliency_map = saliency(score, x, smooth_samples=20, smooth_noise=0.2)\n",
    "saliency_map = normalize(saliency_map)  # Normalize values for visualization\n",
    "\n",
    "# Slightly amplify the saliency map\n",
    "saliency_map_rescaled = (saliency_map[0] - saliency_map[0].min()) / (saliency_map[0].max() - saliency_map[0].min())\n",
    "saliency_map_rescaled = np.clip(saliency_map_rescaled * 1.5, 0, 1)  # Moderate amplification\n",
    "saliency_map_rescaled = np.uint8(255 * saliency_map_rescaled)  # Scale to 0-255\n",
    "\n",
    "# Create a red-highlighted colormap\n",
    "saliency_colormap = np.zeros((*saliency_map_rescaled.shape, 3), dtype=np.uint8)\n",
    "saliency_colormap[..., 0] = saliency_map_rescaled  # Intensified red channel\n",
    "saliency_colormap[..., 1] = 0  # No green\n",
    "saliency_colormap[..., 2] = 0  # No blue\n",
    "\n",
    "# Original image preparation\n",
    "original_image = np.uint8(x[0] * 255)  # Rescale original image to 0-255\n",
    "\n",
    "# Blend original image and saliency map (red highlight)\n",
    "alpha = 0.5  # Lower alpha for subtle emphasis on saliency\n",
    "blended = np.uint8(original_image * (1 - alpha) + saliency_colormap * alpha)\n",
    "\n",
    "# Plot original, saliency map, and overlay\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "\n",
    "# Saliency map\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Saliency Map')\n",
    "plt.imshow(saliency_map[0], cmap='hot')\n",
    "plt.axis('off')\n",
    "\n",
    "# Overlay\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Overlay with Saliency')\n",
    "plt.imshow(blended)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# print(\"Saliency Map Values:\", saliency_map)\n",
    "# print(\"Saliency Map Min:\", saliency_map.min())\n",
    "# print(\"Saliency Map Max:\", saliency_map.max())\n",
    "# prediction = model_ready.predict(x)\n",
    "# print(\"Prediction:\", prediction)\n",
    "# print(\"Input shape:\", x.shape)\n",
    "# print(\"Model input shape:\", model_ready.input_shape)\n",
    "# x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)  # Ensure input is a tf.Tensor\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(x_tensor)  # Watch the input image\n",
    "#     predictions = model_ready(x_tensor)  # Forward pass\n",
    "#     target_class_score = predictions[:, 0]  # Score for the target class (class_index = 0)\n",
    "\n",
    "# grads = tape.gradient(target_class_score, x_tensor)  # Compute gradients w.r.t. input\n",
    "# print(\"Gradients shape:\", grads.shape)\n",
    "# print(\"Gradients min:\", tf.reduce_min(grads).numpy())\n",
    "# print(\"Gradients max:\", tf.reduce_max(grads).numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2024-11-30 13:55:10.321647: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 589ms/step\n",
      "No wrong predictions found.\n"
     ]
    }
   ],
   "source": [
    "# # Load your model\n",
    "# #model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# # model_ready =  keras.models.load_model(r'CNN_ResNet_classic_data.h5')\n",
    "# model_ready =  keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "# # model_ready = keras.models.load_model(r'Baseline_run_on_mask_data.h5')\n",
    "# # model_ready = keras.models.load_model(r'Augmented_run_on_mask_data.h5')\n",
    "# # model_ready = keras.models.load_model(r'Augmented_run_on_classic_data.h5')\n",
    "# # print(\"model_ready.summary()\")\n",
    "# # print(model_ready.summary())\n",
    "# # print(\"model_ready.summary()\")\n",
    "# # prediction = model_ready.predict(x)\n",
    "# # print(\"Predicted class probabilities:\", prediction)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # Prepare arrays to hold the images and labels from the test set\n",
    "# test_images = []\n",
    "# test_labels = []\n",
    "\n",
    "# # Collect images and labels from the test set\n",
    "# for image_batch, label_batch in test_set:\n",
    "#     # Resize images to (224, 224)\n",
    "#     resized_images = tf.image.resize(image_batch, (224, 224))\n",
    "#     test_images.append(resized_images.numpy())  # Convert the batch to numpy array\n",
    "#     test_labels.append(label_batch.numpy())  # Convert the batch to numpy array\n",
    "\n",
    "\n",
    "# # Flatten the list of images and labels into a single array\n",
    "# test_images = np.concatenate(test_images, axis=0)\n",
    "# test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "# # Normalize the images to range [0, 1] (assuming the model expects normalized inputs)\n",
    "# test_images = test_images / 255.0\n",
    "\n",
    "# # Predict the labels using the trained model\n",
    "# predictions = model_ready.predict(test_images)\n",
    "\n",
    "# # Correct predictions: compare the predicted class with the true integer labels\n",
    "# correct_preds = np.argmax(predictions, axis=1) == test_labels\n",
    "\n",
    "# # For best prediction: Highest confidence for the correct predictions\n",
    "# best_confidence = np.max(predictions[correct_preds], axis=1)  # Maximum confidence for correct predictions\n",
    "# best_idx = np.argmax(best_confidence)  # Index of the highest confidence for a correct prediction\n",
    "\n",
    "# # For worst prediction: Lowest confidence for the wrong predictions\n",
    "# wrong_preds = ~correct_preds\n",
    "\n",
    "# # Ensure there are wrong predictions before trying to find the worst one\n",
    "# if np.any(wrong_preds):  # Check if there are any wrong predictions\n",
    "#     worst_confidence = np.max(predictions[wrong_preds], axis=1)  # Maximum confidence for wrong predictions\n",
    "#     worst_idx = np.argmin(worst_confidence)  # Index of the lowest confidence for a wrong prediction\n",
    "#     # Get the worst image and label\n",
    "#     worst_image = test_images[worst_idx]  # Image for worst prediction\n",
    "#     worst_label = test_labels[worst_idx]\n",
    "# else:\n",
    "#     # Handle the case where there are no wrong predictions\n",
    "#     print(\"No wrong predictions found.\")\n",
    "#     worst_image = None\n",
    "#     worst_label = None\n",
    "\n",
    "# # Get the best image and label\n",
    "# best_image = test_images[best_idx]  # Image for best prediction\n",
    "# best_label = test_labels[best_idx]\n",
    "\n",
    "# # Proceed with the rest of the code for saliency visualization...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2024-12-01 10:29:05.203518: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 549ms/step"
     ]
    }
   ],
   "source": [
    "#Don't forget to run preprocessed data before (so that have the right format)\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils import normalize\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to plot the saliency maps\n",
    "def plot_saliency_map(image, model, class_index, smooth_samples=20, smooth_noise=0.2):\n",
    "    # Prepare image for saliency map calculation\n",
    "    x = image.reshape((1,) + image.shape)  # Add batch dimension\n",
    "    x = x / 255.0  # Normalize if your model expects normalized inputs\n",
    "    \n",
    "    # Convert the last activation layer to linear\n",
    "    model.layers[-1].activation = None  # For TensorFlow 2.x compatibility\n",
    "    \n",
    "    # Define the score (class index)\n",
    "    score = CategoricalScore([class_index])\n",
    "    \n",
    "    # Create the Saliency object\n",
    "    saliency = Saliency(model, clone=False)\n",
    "    \n",
    "    # Generate saliency map\n",
    "    saliency_map = saliency(score, x, smooth_samples=smooth_samples, smooth_noise=smooth_noise)\n",
    "    saliency_map = normalize(saliency_map)  # Normalize for visualization\n",
    "    \n",
    "    # Slightly amplify the saliency map\n",
    "    saliency_map_rescaled = (saliency_map[0] - saliency_map[0].min()) / (saliency_map[0].max() - saliency_map[0].min())\n",
    "    saliency_map_rescaled = np.clip(saliency_map_rescaled * 1.5, 0, 1)  # Moderate amplification\n",
    "    saliency_map_rescaled = np.uint8(255 * saliency_map_rescaled)  # Scale to 0-255\n",
    "    \n",
    "    # Create a red-highlighted colormap\n",
    "    saliency_colormap = np.zeros((*saliency_map_rescaled.shape, 3), dtype=np.uint8)\n",
    "    saliency_colormap[..., 0] = saliency_map_rescaled  # Intensified red channel\n",
    "    saliency_colormap[..., 1] = 0  # No green\n",
    "    saliency_colormap[..., 2] = 0  # No blue\n",
    "    \n",
    "    # Original image preparation\n",
    "    original_image = np.uint8(x[0] * 255)  # Rescale original image to 0-255\n",
    "    \n",
    "    # Blend original image and saliency map (red highlight)\n",
    "    alpha = 0.5  # Lower alpha for subtle emphasis on saliency\n",
    "    blended = np.uint8(original_image * (1 - alpha) + saliency_colormap * alpha)\n",
    "    \n",
    "    # Plot original, saliency map, and overlay\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(original_image.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Saliency map\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('Saliency Map')\n",
    "    plt.imshow(saliency_map[0], cmap='hot')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('Overlay with Saliency')\n",
    "    plt.imshow(blended)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the trained model\n",
    "model_ready = keras.models.load_model(r'CNN_ResNet_mask_data.h5')\n",
    "\n",
    "# Prepare the test images and labels (this part should be the same as before)\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for image_batch, label_batch in test_set:\n",
    "    resized_images = tf.image.resize(image_batch, (224, 224))\n",
    "    test_images.append(resized_images.numpy())\n",
    "    test_labels.append(label_batch.numpy())\n",
    "\n",
    "test_images = np.concatenate(test_images, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Predict the labels using the trained model\n",
    "predictions = model_ready.predict(test_images)\n",
    "\n",
    "# Correct predictions: compare the predicted class with the true integer labels\n",
    "correct_preds = np.argmax(predictions, axis=1) == test_labels\n",
    "\n",
    "# Best prediction: Highest confidence for the correct predictions\n",
    "best_confidence = np.max(predictions[correct_preds], axis=1)\n",
    "best_idx = np.argmax(best_confidence)\n",
    "\n",
    "# Worst prediction: Lowest confidence for the wrong predictions\n",
    "wrong_preds = ~correct_preds\n",
    "if np.any(wrong_preds):\n",
    "    worst_confidence = np.max(predictions[wrong_preds], axis=1)\n",
    "    worst_idx = np.argmin(worst_confidence)\n",
    "else:\n",
    "    print(\"No wrong predictions found.\")\n",
    "    worst_image = None\n",
    "    worst_label = None\n",
    "    worst_idx = None\n",
    "\n",
    "# Get the best and worst images and labels\n",
    "best_image = test_images[best_idx]\n",
    "best_label = test_labels[best_idx]\n",
    "\n",
    "if worst_idx is not None:\n",
    "    worst_image = test_images[worst_idx]\n",
    "    worst_label = test_labels[worst_idx]\n",
    "\n",
    "    # Plot saliency map for the worst prediction\n",
    "    print(f\"Plotting saliency map for worst prediction (Index: {worst_idx}, Label: {worst_label})\")\n",
    "    plot_saliency_map(worst_image, model_ready, worst_label)\n",
    "\n",
    "# Plot saliency map for the best prediction\n",
    "print(f\"Plotting saliency map for best prediction (Index: {best_idx}, Label: {best_label})\")\n",
    "plot_saliency_map(best_image, model_ready, best_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
